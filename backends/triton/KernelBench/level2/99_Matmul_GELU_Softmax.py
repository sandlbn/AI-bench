# ruff: noqa: E731
import torch
import torch.nn as nn
import triton
import triton.language as tl


# -----------------------------------------------------------------------------
# Triton kernel: fused float32 Linear (GEMM + bias), AUTOTUNED
# -----------------------------------------------------------------------------
@triton.autotune(
    configs=[
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=4, num_stages=2
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 128, "BLOCK_K": 32}, num_warps=4, num_stages=3
        ),
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 64, "BLOCK_K": 32}, num_warps=4, num_stages=3
        ),
        triton.Config(
            {"BLOCK_M": 128, "BLOCK_N": 128, "BLOCK_K": 32}, num_warps=8, num_stages=3
        ),
        triton.Config(
            {"BLOCK_M": 64, "BLOCK_N": 256, "BLOCK_K": 64}, num_warps=8, num_stages=4
        ),
    ],
    key=["M", "N", "K"],  # autotune per problem size
)
@triton.jit
def _linear_fp32_kernel(
    x_ptr,  # pointer to X [M, K], float32
    w_ptr,  # pointer to W [N, K], float32 (we index as W^T)
    b_ptr,  # pointer to bias [N], float32
    y_ptr,  # pointer to output Y [M, N], float32
    M,
    N,
    K,  # matrix dimensions
    stride_xm,
    stride_xk,  # strides for X
    stride_wk,
    stride_wn,  # strides for W^T  (we will index W as transposed)
    stride_ym,
    stride_yn,  # strides for Y
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    m_start = pid_m * BLOCK_M
    n_start = pid_n * BLOCK_N

    offs_m = m_start + tl.arange(0, BLOCK_M)
    offs_n = n_start + tl.arange(0, BLOCK_N)

    mask_m = offs_m < M
    mask_n = offs_n < N

    # accumulator
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # iterate over K in chunks
    k_tiles = tl.cdiv(K, BLOCK_K)
    off_k = tl.arange(0, BLOCK_K)

    for kt in range(k_tiles):
        k_start = kt * BLOCK_K
        offs_k = k_start + off_k
        mask_k = offs_k < K

        # X block ptrs: [BLOCK_M, BLOCK_K]
        x_ptrs = x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk

        # W^T block ptrs: [BLOCK_K, BLOCK_N]
        w_ptrs = w_ptr + offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn

        # load
        x_block = tl.load(
            x_ptrs,
            mask=(mask_m[:, None] & mask_k[None, :]),
            other=0.0,
        )
        w_block = tl.load(
            w_ptrs,
            mask=(mask_k[:, None] & mask_n[None, :]),
            other=0.0,
        )

        # fma
        acc += tl.dot(x_block, w_block)

    # add bias
    b_vals = tl.load(b_ptr + offs_n, mask=mask_n, other=0.0)
    acc += b_vals[None, :]

    # store
    y_ptrs = y_ptr + offs_m[:, None] * stride_ym + offs_n[None, :] * stride_yn
    mask = mask_m[:, None] & mask_n[None, :]
    tl.store(y_ptrs, acc, mask=mask)


# -----------------------------------------------------------------------------
# Triton kernel: GELU + rowâ€wise Softmax on float32
# -----------------------------------------------------------------------------
@triton.jit
def _softmax_gelu_fp32_kernel(
    x_ptr,
    y_ptr,
    M,
    N,
    stride_xm,
    stride_xn,
    stride_ym,
    stride_yn,
    BLOCK: tl.constexpr,
):
    row = tl.program_id(0)
    offs = tl.arange(0, BLOCK)
    inv_sqrt2 = 0.7071067811865476

    # Stage 1: compute max over GELU(x) in the row
    max_val = -1e20
    start = 0
    while start < N:
        idx = start + offs
        mask = idx < N

        ptrs = x_ptr + row * stride_xm + idx * stride_xn
        x = tl.load(ptrs, mask=mask, other=0.0)

        # GELU
        gate = 0.5 * (1.0 + tl.erf(x * inv_sqrt2))
        x_gelu = x * gate

        # block max
        block_max = tl.max(x_gelu, axis=0)
        max_val = tl.maximum(block_max, max_val)

        start += BLOCK

    # Stage 2: compute sum of exp(GELU(x) - max_val)
    sum_val = 0.0
    start = 0
    while start < N:
        idx = start + offs
        mask = idx < N

        ptrs = x_ptr + row * stride_xm + idx * stride_xn
        x = tl.load(ptrs, mask=mask, other=0.0)

        gate = 0.5 * (1.0 + tl.erf(x * inv_sqrt2))
        x_gelu = x * gate

        exp_x = tl.exp(x_gelu - max_val)
        sum_val = sum_val + tl.sum(exp_x, axis=0)

        start += BLOCK

    inv_sum = 1.0 / sum_val

    # Stage 3: write softmax outputs
    start = 0
    while start < N:
        idx = start + offs
        mask = idx < N

        in_ptrs = x_ptr + row * stride_xm + idx * stride_xn
        out_ptrs = y_ptr + row * stride_ym + idx * stride_yn

        x = tl.load(in_ptrs, mask=mask, other=0.0)
        gate = 0.5 * (1.0 + tl.erf(x * inv_sqrt2))
        x_gelu = x * gate

        exp_x = tl.exp(x_gelu - max_val)
        y = exp_x * inv_sum

        tl.store(out_ptrs, y, mask=mask)

        start += BLOCK


# -----------------------------------------------------------------------------
# Low-level fused Triton wrapper (XPU, dtype-flexible)
# -----------------------------------------------------------------------------
def _fused_linear_gelu_softmax_xpu(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
) -> torch.Tensor:
    """
    Fused X @ W^T + bias + GELU + softmax over last dim.

    Assumes:
      - x, weight, bias are on XPU.
      - Any floating dtype (fp32/fp16/bf16...) is allowed; compute in fp32,
        then cast back to x.dtype.

    Shapes:
      - x:      [M, K]
      - weight: [N, K]
      - bias:   [N]
      - out:    [M, N]
    """
    if x.device.type != "xpu":
        raise RuntimeError(f"Expected x on 'xpu', got {x.device}")

    if not (
        x.is_floating_point()
        and weight.is_floating_point()
        and bias.is_floating_point()
    ):
        raise TypeError("x, weight, and bias must be floating point tensors")

    if weight.device != x.device or bias.device != x.device:
        raise RuntimeError("x, weight, and bias must be on the same device")

    if x.ndim != 2:
        raise ValueError(f"Expected x.ndim == 2, got {x.ndim}")
    if weight.ndim != 2 or bias.ndim != 1:
        raise ValueError("Expected weight.ndim == 2 and bias.ndim == 1")

    M, K = x.shape
    N, Kw = weight.shape

    if Kw != K:
        raise ValueError(f"Weight K dim {Kw} != x K dim {K}")
    if bias.shape[0] != N:
        raise ValueError(f"Bias length {bias.shape[0]} != output dim {N}")

    # Preserve original dtype for final output
    orig_dtype = x.dtype

    # Work in fp32 for numerical stability
    x32 = x.to(torch.float32).contiguous()
    w32 = weight.to(torch.float32).contiguous()
    b32 = bias.to(torch.float32).contiguous()

    # Linear: X @ W^T + bias
    y_lin = torch.empty((M, N), dtype=torch.float32, device=x.device)

    # Autotuned grid based on selected BLOCK_M / BLOCK_N
    grid_lin = lambda META: (
        triton.cdiv(M, META["BLOCK_M"]),
        triton.cdiv(N, META["BLOCK_N"]),
    )

    _linear_fp32_kernel[grid_lin](
        x32,
        w32,
        b32,
        y_lin,
        M,
        N,
        K,
        x32.stride(0),
        x32.stride(1),
        w32.stride(1),
        w32.stride(0),
        y_lin.stride(0),
        y_lin.stride(1),
    )

    # GELU + Softmax
    y_out32 = torch.empty_like(y_lin)
    BLOCK = 256
    grid_sm = (M,)

    _softmax_gelu_fp32_kernel[grid_sm](
        y_lin,
        y_out32,
        M,
        N,
        y_lin.stride(0),
        y_lin.stride(1),
        y_out32.stride(0),
        y_out32.stride(1),
        BLOCK=BLOCK,
    )

    # Cast back to the original dtype that KernelBench requested
    return y_out32.to(orig_dtype)


# -----------------------------------------------------------------------------
# KernelBench-compatible Model wrapper (weights/bias embedded)
# -----------------------------------------------------------------------------
class Model(nn.Module):
    """
    KernelBench-compatible wrapper for fused:

        y = softmax( GELU( X @ W^T + b ) )

    """

    def __init__(self, in_features: int, out_features: int):
        super(Model, self).__init__()
        # embed weight and bias as a Linear module;
        # we only use its .weight and .bias in Triton.
        self.linear = nn.Linear(in_features, out_features)

    def forward(self, X: torch.Tensor) -> torch.Tensor:
        """
        X: [BATCH, IN_FEAT]
        Returns: [BATCH, OUT_FEAT]
        """
        if not (hasattr(torch, "xpu") and torch.xpu.is_available()):
            raise RuntimeError("XPU is not available; TRITON backend is XPU-only")
        if X.device.type != "xpu":
            raise RuntimeError(f"Expected X on 'xpu', got {X.device}")

        # Extract embedded parameters (already moved to correct device/dtype by .to(...))
        weight = self.linear.weight  # [OUT_FEAT, IN_FEAT] = [N, K]
        bias = self.linear.bias  # [OUT_FEAT] = [N]

        return _fused_linear_gelu_softmax_xpu(X, weight, bias)
